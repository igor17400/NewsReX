# @package _global_

defaults:
  - /sampling/base
  - _self_
  - override /model: naml
  - override /dataset: japanese

# Experiment name
name: "naml_japanese"
description: "NAML model trained on Japanese news dataset"
model_name: "NAML"

# Global parameters
seed: 42 # Global seed for reproducibility
max_title_length: 40  # Slightly longer for Japanese text
max_history_length: 50
max_impressions_length: 5
max_abstract_length: 60  # Longer for Japanese

# Dataset specific
dataset:
  name: "japanese"
  version: "v1"
  language: "japanese"
  embedding_type: "bpemb"  # Use BPEmb for Japanese
  embedding_size: 300
  max_title_length: ${max_title_length}
  max_abstract_length: ${max_abstract_length}
  max_history_length: ${max_history_length}
  random_train_samples: false
  validation_split_strategy: "random" # Options: chronological, random
  process_title: true
  process_abstract: true
  process_category: true
  process_subcategory: true

# Model specific
model:
  embedding_size: 300 # word embedding size
  category_embedding_dim: 100
  subcategory_embedding_dim: 100
  cnn_filter_num: 400
  cnn_kernel_size: 3
  word_attention_query_dim: 200
  view_attention_query_dim: 200
  user_attention_query_dim: 200
  dropout_rate: 0.2
  loss:
    name: "categorical_crossentropy" # Options: categorical_crossentropy, binary_crossentropy
    # 'from_logits' specifies whether the model's output is raw logits (before activation) or probabilities (after softmax/sigmoid).
    # Set to 'false' if your model outputs probabilities (e.g., after softmax or sigmoid).
    # Set to 'true' if your model outputs raw, unnormalized scores (logits) and you want the loss function to apply the activation internally.
    from_logits: false # Whether the model outputs logits or probabilities
    reduction: "sum_over_batch_size"
    label_smoothing: 0.0

# Training parameters
train:
  batch_size: 16
  num_epochs: 5
  learning_rate: 0.0001
  early_stopping:
    patience: 3
    min_improvement: 0.01

# Evaluation parameters
eval:
  fast_evaluation: true # Whether to use fast evaluation with precomputed vectors
  batch_size: 1024 # Batch size for evaluation

# Logging
logging:
  enable_wandb: true
  project_name: "NewsReX"
  experiment_name: "naml_japanese"

# Sampling configuration
sampling:
  max_impressions_length: 5
  strategy: "random"
  random:
    replace: false

device:
  gpu_ids: [ 0 ] # Use first GPU
  precision: "float32"