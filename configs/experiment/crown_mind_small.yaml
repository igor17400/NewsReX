# @package _global_

defaults:
- /sampling/base
- _self_
- override /model: crown
- override /dataset: mind

# Experiment name
name: "crown_mind_small"
description: "CROWN model trained on MIND-small dataset"
model_name: "CROWN"

# Global parameters
seed: 42 # Global seed for reproducibility
max_title_length: 32
max_abstract_length: 50
max_history_length: 50
max_impressions_length: 5

# Dataset specific
dataset:
  name: "mind"
  version: "small"
  max_title_length: ${max_title_length}
  max_abstract_length: ${max_abstract_length}
  max_history_length: ${max_history_length}
  random_train_samples: true
  validation_split_strategy: "random" # Options: chronological, random
  process_title: true
  process_abstract: true # CROWN uses both title and abstract
  process_category: true
  process_subcategory: true

# Model specific
model:
  embedding_size: 300
  
  # Intent disentanglement parameters
  intent_num: 3 # Number of intents (k parameter)
  intent_embedding_dim: 200
  alpha: 0.3 # Weight for auxiliary category prediction loss
  
  # Category embeddings
  category_embedding_dim: 100
  subcategory_embedding_dim: 100
  
  # Attention
  attention_dim: 200
  
  # Transformer parameters
  num_heads: 12
  head_dim: 25
  feedforward_dim: 512
  num_layers: 2
  
  # GNN parameters
  gnn_type: 'graphsage'  # Options: 'graphsage' or 'gat'
  graph_hidden_dim: 300
  graph_num_layers: 1
  
  # GAT-specific parameters (used when gnn_type='gat')
  gat_num_heads: 4
  gat_alpha: 0.2  # LeakyReLU negative slope for attention mechanism
  gat_concat_heads: true  # Whether to concatenate or average multi-head outputs
  
  # GraphSAGE-specific parameters (used when gnn_type='graphsage')
  sage_aggregator: 'mean'  # Options: 'mean', 'max', 'sum', 'attention'
  sage_normalize: true  # Whether to L2-normalize the output embeddings
  
  # Common parameters
  dropout_rate: 0.2
  max_title_length: ${max_title_length}
  max_abstract_length: ${max_abstract_length}
  max_history_length: ${max_history_length}
  max_impressions_length: ${max_impressions_length}
  
  # Processing flags
  process_user_id: false # CROWN doesn't use user IDs like LSTUR
  process_abstract: true # CROWN uses both title and body
  process_category: true # CROWN uses category information
  process_subcategory: true # CROWN uses subcategory information
  
  loss:
    name: "crown_combined" # Combined loss with auxiliary category prediction
    alpha: 0.3 # Weight for auxiliary category prediction loss (same as intent alpha)
    num_categories: 18 # Number of news categories in MIND dataset
    reduction: "sum_over_batch_size"
    label_smoothing: 0.0

# Training parameters
train:
  batch_size: 16
  num_epochs: 10
  learning_rate: 0.0001
  early_stopping:
    patience: 5
    min_improvement: 0.01

# Logging
logging:
  enable_wandb: false
  project_name: "NewsReX"
  experiment_name: crown_mind_small_bs${train.batch_size}
  log_every_n_steps: 100

# Sampling configuration
sampling:
  max_impressions_length: 5
  strategy: "random"
  random:
    replace: false

device:
  gpu_ids: [ 0 ] # Use first GPU
  precision: "float16"  # Options: "float32", "float16", "bfloat16"