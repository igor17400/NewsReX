# @package _global_

defaults:
- /sampling/base
- _self_
- override /model: lstur
- override /dataset: mind

# Experiment name
name: "lstur_mind_small"
description: "LSTUR model trained on MIND-small dataset"
model_name: "LSTUR"

# Global parameters
seed: 42 # Global seed for reproducibility
max_title_length: 32
max_history_length: 50
max_impressions_length: 5

# Dataset specific
dataset:
  name: "mind"
  version: "small"
  max_title_length: ${max_title_length}
  max_history_length: ${max_history_length}
  random_train_samples: true
  validation_split_strategy: "random" # Options: chronological, random
  process_title: true
  process_user_id: true
  process_category: true
  process_subcategory: true

# Model specific
model:
  embedding_size: 300
  user_embedding_dim: 400
  cnn_filter_num: 400
  cnn_kernel_size: 3
  category_embedding_dim: 100
  subcategory_embedding_dim: 100
  attention_hidden_dim: 200
  dropout_rate: 0.2
  cnn_activation: "relu"
  num_users: 94057 # Total number of users in the datasets (train + validation + test)
  user_representation_type: "gru" # Options: "lstm" or "gru"
  user_combination_type: "ini" # Options: "ini" or "con"
  loss:
    name: "categorical_crossentropy" # Options: categorical_crossentropy, binary_crossentropy
    from_logits: false # Whether the model outputs logits or probabilities
    reduction: "sum_over_batch_size"
    label_smoothing: 0.0

# Training parameters
train:
  batch_size: 64
  num_epochs: 10
  learning_rate: 0.0001
  early_stopping:
    patience: 5
    min_improvement: 0.01

# Logging
logging:
  enable_wandb: true
  project_name: "BTC"
  experiment_name: lstur_mind_small
  log_every_n_steps: 100

# Sampling configuration
sampling:
  max_impressions_length: 5
  strategy: "random"
  random:
    replace: false

device:
  gpu_ids: [ 0 ] # Use first GPU
  mixed_precision: false
